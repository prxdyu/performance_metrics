{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c9d9e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e11a8550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data\n",
    "df=pd.read_csv('5_a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f14f949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chaning the class labels\n",
    "df['y_pred']=np.where(df['proba']<0.5,0,1)\n",
    "\n",
    "# changing the datatype of the y column as int\n",
    "df['y']=df['y'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e921402",
   "metadata": {},
   "source": [
    "#### 1. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5b194c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_confusion_matrix(x,y):\n",
    "    \n",
    "    # checking whether the given two series has same length\n",
    "    if len(x)==len(y):\n",
    "        \n",
    "        # initializing TP,FP,TN,FN\n",
    "        TP=0\n",
    "        TN=0\n",
    "        FP=0\n",
    "        FN=0\n",
    "\n",
    "        # iterating through each row and  and \n",
    "        for i in range(len(x)):\n",
    "\n",
    "            # finding the true label\n",
    "            y_true=x[i]\n",
    "\n",
    "            # finding the predicted label\n",
    "            y_pred=y[i]\n",
    "\n",
    "            # decing whether its TP or FP or TN or FN\n",
    "            tp_condition= (y_true==1)&(y_pred==1)\n",
    "            tn_condition= (y_true==0)&(y_pred==0)\n",
    "            fp_condition= (y_true==0)&(y_pred==1)\n",
    "            fn_condition= (y_true==1)&(y_pred==0)\n",
    "\n",
    "            if tp_condition:\n",
    "                TP+=1\n",
    "\n",
    "            elif tn_condition:\n",
    "                TN+=1\n",
    "\n",
    "            elif fp_condition:\n",
    "                FP+=1\n",
    "\n",
    "            elif fn_condition:\n",
    "                FN+=1\n",
    "\n",
    "        # building our confusion matrix\n",
    "        conf_mat=np.array([[TN,FP],[FN,TP]])\n",
    "\n",
    "        # returning the confusion matrix\n",
    "        return conf_mat\n",
    "    \n",
    "    else:\n",
    "        print('x and y has different length, please provide the series with same length')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09ac3f4",
   "metadata": {},
   "source": [
    "#### 2. F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10829163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(x,y):\n",
    "    \n",
    "    # checking whether the given two series has same length\n",
    "    if len(x)==len(y):\n",
    "        \n",
    "        # initializing TP,FP,TN,FN\n",
    "        TP=0\n",
    "        TN=0\n",
    "        FP=0\n",
    "        FN=0\n",
    "\n",
    "        # iterating through each row and  and \n",
    "        for i in range(len(x)):\n",
    "\n",
    "            # finding the true label\n",
    "            y_true=x[i]\n",
    "\n",
    "            # finding the predicted label\n",
    "            y_pred=y[i]\n",
    "\n",
    "            # decing whether its TP or FP or TN or FN\n",
    "            tp_condition= (y_true==1)&(y_pred==1)\n",
    "            tn_condition= (y_true==0)&(y_pred==0)\n",
    "            fp_condition= (y_true==0)&(y_pred==1)\n",
    "            fn_condition= (y_true==1)&(y_pred==0)\n",
    "\n",
    "            if tp_condition:\n",
    "                TP+=1\n",
    "\n",
    "            elif tn_condition:\n",
    "                TN+=1\n",
    "\n",
    "            elif fp_condition:\n",
    "                FP+=1\n",
    "\n",
    "            elif fn_condition:\n",
    "                FN+=1\n",
    "        \n",
    "        # calculating precision\n",
    "        precision=TP/(TP+FP)\n",
    "        \n",
    "        # calulating recall\n",
    "        recall=TP/(TP+FN)\n",
    "        \n",
    "        # calculating the f1 score\n",
    "        f_score=2*precision*recall/(precision+recall)\n",
    "\n",
    "        # returning the confusion matrix\n",
    "        return f_score\n",
    "    \n",
    "    else:\n",
    "        print('x and y has different length, please provide the series with same length')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d460cea",
   "metadata": {},
   "source": [
    "#### 3. Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fa33904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(x,y):\n",
    "    \n",
    "    # checking whether the given two series has same length\n",
    "    if len(x)==len(y):\n",
    "        \n",
    "        # initializing TP,FP,TN,FN\n",
    "        TP=0\n",
    "        TN=0\n",
    "        FP=0\n",
    "        FN=0\n",
    "\n",
    "        # iterating through each row and  and \n",
    "        for i in range(len(x)):\n",
    "\n",
    "            # finding the true label\n",
    "            y_true=x[i]\n",
    "\n",
    "            # finding the predicted label\n",
    "            y_pred=y[i]\n",
    "\n",
    "            # decing whether its TP or FP or TN or FN\n",
    "            tp_condition= (y_true==1)&(y_pred==1)\n",
    "            tn_condition= (y_true==0)&(y_pred==0)\n",
    "            fp_condition= (y_true==0)&(y_pred==1)\n",
    "            fn_condition= (y_true==1)&(y_pred==0)\n",
    "\n",
    "            if tp_condition:\n",
    "                TP+=1\n",
    "\n",
    "            elif tn_condition:\n",
    "                TN+=1\n",
    "\n",
    "            elif fp_condition:\n",
    "                FP+=1\n",
    "\n",
    "            elif fn_condition:\n",
    "                FN+=1\n",
    "        \n",
    "        # calculating the accuracy\n",
    "        accuracy=(TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "        # returning the confusion matrix\n",
    "        return accuracy\n",
    "    \n",
    "    else:\n",
    "        print('x and y has different length, please provide the series with same length')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08b58ff",
   "metadata": {},
   "source": [
    "#### 4.  Mean Squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bd5f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_eror(x,y):\n",
    "    errors=[]\n",
    "    \n",
    "    # iterating through each row in the series\n",
    "    for i in x.index:\n",
    "        y_true=x[i]\n",
    "        y_pred=y[i]\n",
    "        error=(y_true-y_pred)**2\n",
    "        errors.append(error)\n",
    "    \n",
    "    # calculating the mean squared error\n",
    "    total_squared_error=sum(errors)\n",
    "    mean_sq_error=total_squared_error/len(x)\n",
    "    \n",
    "    print(mean_sq_error)\n",
    "    return mean_sq_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8884c1",
   "metadata": {},
   "source": [
    "#### 5. Mean absolute percentage error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8723285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(x,y):\n",
    "    \n",
    "    # making an empty list for erros for all datapoints\n",
    "    errors=[]\n",
    "    \n",
    "    for i in x.index:\n",
    "        y_true=x[i]\n",
    "        y_pred=y[i]\n",
    "        \n",
    "        # calculating the error\n",
    "        error=abs(y_true-y_pred)\n",
    "        errors.append(error)\n",
    "    \n",
    "    # computing the total error\n",
    "    total_error=sum(errors)\n",
    "    total_pred=y.sum()\n",
    "    \n",
    "    return total_error/total_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e1216",
   "metadata": {},
   "source": [
    "#### 6. R^2 Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25df0a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rss(x,y):\n",
    "    errors=[]\n",
    "    \n",
    "    # iterating through each row in the series\n",
    "    for i in x.index:\n",
    "        y_true=x[i]\n",
    "        y_pred=y[i]\n",
    "        error=(y_true-y_pred)**2\n",
    "        errors.append(error)\n",
    "    \n",
    "    # calculating the mean squared error\n",
    "    total_squared_error=sum(errors)\n",
    "    return total_squared_error\n",
    "\n",
    "def tss(x,y):\n",
    "    errors=[]\n",
    "    \n",
    "    # iterating through each row in the series\n",
    "    for i in x.index:\n",
    "        y_true=x[i]\n",
    "        y_pred=y.mean()\n",
    "        error=(y_true-y_pred)**2\n",
    "        errors.append(error)\n",
    "    \n",
    "    # calculating the mean squared error\n",
    "    total_squared_error=sum(errors)\n",
    "    \n",
    "    return total_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30cf40cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_value(x,y):\n",
    "    e=rss(x,y)/tss(x,y)\n",
    "    return 1-e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7cb903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
